{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8129f7e",
   "metadata": {},
   "source": [
    "# Pig Detection CNN Project Template\n",
    "\n",
    "This notebook scaffolds the workflow for the HW1 dense pig detection assignment. Replace each `### TODO ###` with your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e93e6",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2107e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ### TODO ###: add any additional third-party libraries (albumentations, matplotlib, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79769ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO ###: adjust experiment metadata\n",
    "EXPERIMENT_NAME = \"pig_detection_baseline\"\n",
    "OUTPUT_DIR = Path(\"artifacts\") / EXPERIMENT_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ### TODO ###: configure dataset root and annotation paths\n",
    "DATA_ROOT = Path(\"../HW1/data\")\n",
    "ANNOTATIONS_PATH = DATA_ROOT / \"annotations.json\"\n",
    "IMAGES_DIR = DATA_ROOT / \"images\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e21887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ### TODO ###: choose an appropriate seed value\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30f439",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PigDetectionDataset(Dataset):\n",
    "    def __init__(self, annotations, images_dir: Path, transforms=None):\n",
    "        self.annotations = annotations\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, annotations_path: Path, images_dir: Path, transforms=None):\n",
    "        # ### TODO ###: parse the annotation format used in the assignment\n",
    "        with open(annotations_path, \"r\") as fp:\n",
    "            annotations = json.load(fp)\n",
    "        return cls(annotations, images_dir, transforms)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # ### TODO ###: return dataset size\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        record = self.annotations[idx]\n",
    "        # ### TODO ###: load image and target (bounding boxes, labels, etc.)\n",
    "        image_path = self.images_dir / record.get(\"file_name\", \"\")\n",
    "        # ### TODO ###: replace placeholder with actual image loading logic\n",
    "        image = None\n",
    "        target = {}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# ### TODO ###: build any required preprocessing / augmentation transforms\n",
    "train_transforms = None\n",
    "val_transforms = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size: int = 4, num_workers: int = 4):\n",
    "    # ### TODO ###: decide how to split annotations into train/val/test\n",
    "    dataset = PigDetectionDataset.from_json(ANNOTATIONS_PATH, IMAGES_DIR, transforms=train_transforms)\n",
    "\n",
    "    # ### TODO ###: implement a proper split instead of reusing the entire dataset\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ### TODO ###: set batch size and worker count\n",
    "train_loader, val_loader = create_dataloaders(batch_size=4, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8092172",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af518f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PigDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ### TODO ###: define the CNN backbone and detection heads\n",
    "        self.feature_extractor = nn.Identity()\n",
    "        self.head = nn.Identity()\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # ### TODO ###: implement forward pass and return outputs compatible with loss\n",
    "        return {}\n",
    "\n",
    "def build_model() -> PigDetectionModel:\n",
    "    model = PigDetectionModel()\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO ###: define your loss function(s) and metrics\n",
    "def compute_loss(outputs, targets):\n",
    "    # Placeholder example loss\n",
    "    loss = torch.tensor(0.0, device=DEVICE)\n",
    "    return loss\n",
    "\n",
    "# ### TODO ###: add metric computations (mAP, IoU, etc.)\n",
    "def compute_metrics(outputs, targets):\n",
    "    metrics = {}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df92c8",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e32e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO ###: configure optimizer, LR scheduler, and training hyperparameters\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = None  # ### TODO ###: optionally add LR scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afb4d2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bf8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(dataloader):\n",
    "        # ### TODO ###: move images/targets to DEVICE and preprocess as needed\n",
    "        outputs = model(images, targets)\n",
    "        loss = compute_loss(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # ### TODO ###: add logging / visualization hooks\n",
    "    return running_loss / max(1, len(dataloader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    aggregated_metrics = {}\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        outputs = model(images, targets)\n",
    "        loss = compute_loss(outputs, targets)\n",
    "        eval_loss += loss.item()\n",
    "        metrics = compute_metrics(outputs, targets)\n",
    "        # ### TODO ###: accumulate metrics (mean IoU, mAP, etc.)\n",
    "\n",
    "    eval_loss /= max(1, len(dataloader))\n",
    "    # ### TODO ###: finalize aggregated metrics\n",
    "    return eval_loss, aggregated_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_metric=None):\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"best_metric\": best_metric,\n",
    "    }\n",
    "    ckpt_path = OUTPUT_DIR / f\"checkpoint_epoch_{epoch:03d}.pth\"\n",
    "    torch.save(checkpoint, ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path: Path):\n",
    "    state = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state\"])\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO ###: configure early stopping / model selection criteria\n",
    "best_val_metric = None\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_metrics\": []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_metrics = evaluate(model, val_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_metrics\"].append(val_metrics)\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "    # ### TODO ###: print or log validation metrics\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    # ### TODO ###: implement best model tracking based on desired metric\n",
    "    save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "# ### TODO ###: persist training history (e.g., to JSON/CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c967b",
   "metadata": {},
   "source": [
    "## Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d33bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO ###: generate qualitative visualizations for predictions\n",
    "def visualize_predictions(images, targets, outputs):\n",
    "    pass\n",
    "\n",
    "# ### TODO ###: load best checkpoint before running final evaluation\n",
    "# state = load_checkpoint(model, optimizer, OUTPUT_DIR / \"best.pth\")\n",
    "# val_loss, val_metrics = evaluate(model, val_loader)\n",
    "# print(val_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5baea",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(model, image):\n",
    "    model.eval()\n",
    "    # ### TODO ###: preprocess single image and run forward pass\n",
    "    outputs = model(image)\n",
    "    # ### TODO ###: postprocess outputs into bounding boxes / scores\n",
    "    return outputs\n",
    "\n",
    "# ### TODO ###: add inference-on-directory or video helpers if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c490fd",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4015085",
   "metadata": {},
   "source": [
    "- Replace each `### TODO ###` with assignment-specific code for data parsing, model architecture, and evaluation.\n",
    "- Add experiment tracking (TensorBoard, Weights & Biases, etc.) if desired.\n",
    "- Integrate assignment-specific metrics and submission formatting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
