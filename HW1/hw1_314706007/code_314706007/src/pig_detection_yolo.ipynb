{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba67c98",
   "metadata": {},
   "source": [
    "\n",
    "# Pig Detection with YOLOv8 (Training from Scratch)\n",
    "\n",
    "This notebook prepares the TAICa pig detection dataset for YOLOv8, trains a detector **from random initialization (no pretrained weights)**, evaluates it, and exports predictions that follow the submission format described in the homework slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c59de",
   "metadata": {},
   "source": [
    "\n",
    "**Setup Notes**\n",
    "- Keep the original dataset folder `taica-cvpdl-2025-hw-1` in the homework root (three levels up from this notebook).\n",
    "- Review `sample_submission.csv` to verify the expected `PredictionString` layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d488c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in a clean environment, uncomment the line below to install dependencies.\n",
    "# %pip install ultralytics==8.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b60480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5645661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory: /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "DATA_ROOT = (PROJECT_DIR / '../../../taica-cvpdl-2025-hw-1').resolve()\n",
    "TRAIN_IMG_DIR = DATA_ROOT / 'train' / 'img'\n",
    "TEST_IMG_DIR = DATA_ROOT / 'test' / 'img'\n",
    "GT_PATH = DATA_ROOT / 'train' / 'gt.txt'\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(f'Dataset folder not found at {DATA_ROOT}. Update DATA_ROOT if your layout differs.')\n",
    "\n",
    "EXPERIMENT_NAME = 'pig_detection_yolo'\n",
    "RUN_ID = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "EXPERIMENT_DIR = (PROJECT_DIR / 'artifacts' / EXPERIMENT_NAME / RUN_ID).resolve()\n",
    "YOLO_DATA_DIR = EXPERIMENT_DIR / 'dataset'\n",
    "RUNS_DIR = EXPERIMENT_DIR / 'runs'\n",
    "\n",
    "YOLO_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Experiment directory: {EXPERIMENT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036218fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_annotations(gt_path: Path) -> Dict[int, List[Tuple[float, float, float, float]]]:\n",
    "    annotations: Dict[int, List[Tuple[float, float, float, float]]] = defaultdict(list)\n",
    "    with gt_path.open('r') as fh:\n",
    "        for line in fh:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            image_id = int(parts[0])\n",
    "            x, y, w, h = map(float, parts[1:5])\n",
    "            annotations[image_id].append((x, y, w, h))\n",
    "    if not annotations:\n",
    "        raise RuntimeError(f'No annotations parsed from {gt_path}.')\n",
    "    return annotations\n",
    "\n",
    "def convert_bbox_to_yolo(x: float, y: float, w: float, h: float, img_w: int, img_h: int):\n",
    "    xc = (x + w / 2.0) / img_w\n",
    "    yc = (y + h / 2.0) / img_h\n",
    "    ww = w / img_w\n",
    "    hh = h / img_h\n",
    "    xc = min(max(xc, 0.0), 1.0)\n",
    "    yc = min(max(yc, 0.0), 1.0)\n",
    "    ww = min(max(ww, 0.0), 1.0)\n",
    "    hh = min(max(hh, 0.0), 1.0)\n",
    "    return xc, yc, ww, hh\n",
    "\n",
    "def prepare_dataset(images_dir: Path, annotations: Dict[int, List[Tuple[float, float, float, float]]], output_dir: Path, val_ratio: float = 0.1, seed: int = 42):\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "    for split in ['train', 'val']:\n",
    "        (output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "        (output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_ids = sorted(annotations.keys())\n",
    "    available_ids = []\n",
    "    missing_ids = []\n",
    "    for image_id in all_ids:\n",
    "        img_name = f'{image_id:08d}.jpg'\n",
    "        if (images_dir / img_name).exists():\n",
    "            available_ids.append(image_id)\n",
    "        else:\n",
    "            missing_ids.append(image_id)\n",
    "\n",
    "    if missing_ids:\n",
    "        sample = ', '.join(f'{mid:08d}' for mid in missing_ids[:5])\n",
    "        print(f'Skipping {len(missing_ids)} annotation entries without images. Examples: {sample}')\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(available_ids)\n",
    "    val_count = max(1, int(len(available_ids) * val_ratio)) if available_ids else 0\n",
    "    val_ids = set(available_ids[:val_count])\n",
    "\n",
    "    stats = {'train': 0, 'val': 0, 'boxes': 0, 'skipped': len(missing_ids)}\n",
    "\n",
    "    for image_id in available_ids:\n",
    "        split = 'val' if image_id in val_ids else 'train'\n",
    "        img_name = f'{image_id:08d}.jpg'\n",
    "        src_path = images_dir / img_name\n",
    "        with Image.open(src_path) as img:\n",
    "            img_w, img_h = img.size\n",
    "\n",
    "        dst_img = output_dir / 'images' / split / img_name\n",
    "        shutil.copy2(src_path, dst_img)\n",
    "\n",
    "        label_lines = []\n",
    "        for bbox in annotations[image_id]:\n",
    "            xc, yc, ww, hh = convert_bbox_to_yolo(*bbox, img_w=img_w, img_h=img_h)\n",
    "            label_lines.append(f'0 {xc:.6f} {yc:.6f} {ww:.6f} {hh:.6f}')\n",
    "        label_path = output_dir / 'labels' / split / f'{image_id:08d}.txt'\n",
    "        label_path.write_text('\\n'.join(label_lines))\n",
    "        stats[split] += 1\n",
    "        stats['boxes'] += len(label_lines)\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4b9cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 4 annotation entries without images. Examples: 00000604, 00000605, 00000606, 00000607\n",
      "Dataset prepared at /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/dataset\n",
      "train images: 1140 | val images: 126 | total boxes: 38619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "annotations = load_annotations(GT_PATH)\n",
    "stats = prepare_dataset(TRAIN_IMG_DIR, annotations, YOLO_DATA_DIR, val_ratio=0.1, seed=SEED)\n",
    "print(f'Dataset prepared at {YOLO_DATA_DIR}')\n",
    "print(f\"train images: {stats['train']} | val images: {stats['val']} | total boxes: {stats['boxes']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3787fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO data config saved to /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/pig_dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_CONFIG_PATH = EXPERIMENT_DIR / 'pig_dataset.yaml'\n",
    "config_lines = [\n",
    "    f'path: {YOLO_DATA_DIR.as_posix()}',\n",
    "    'train: images/train',\n",
    "    'val: images/val',\n",
    "    'nc: 1',\n",
    "    'names:',\n",
    "    '  0: pig',\n",
    "]\n",
    "DATA_CONFIG_PATH.write_text('\\n'.join(config_lines) + '\\n')\n",
    "print(f'YOLO data config saved to {DATA_CONFIG_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.209 ğŸš€ Python-3.11.11 torch-2.8.0 CPU (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/pig_dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_scratch, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=False, profile=False, project=/Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/runs/yolov8n_scratch, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 163.4Â±40.7 MB/s, size: 27.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/dataset/labels/train... 1140 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1140/1140 5.6Kit/s 0.2s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 137.4Â±28.3 MB/s, size: 22.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/dataset/labels/val... 126 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 126/126 4.8Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/dataset/labels/val.cache\n",
      "Plotting labels to /Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/runs/yolov8n_scratch/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/daniel/Desktop/Study/Computer Vision/HW1/hw1_314706007/code_314706007/src/artifacts/pig_detection_yolo/20251010-211459/runs/yolov8n_scratch\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100         0G       4.69      3.637      4.235       1120        640: 1% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1/72 0.0it/s 14.1s<29:60"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_CONFIG = 'yolov8n.yaml'\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 640\n",
    "RUN_NAME = 'yolov8n_scratch'\n",
    "\n",
    "model = YOLO(MODEL_CONFIG)\n",
    "train_results = model.train(\n",
    "    data=str(DATA_CONFIG_PATH),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    project=str(RUNS_DIR),\n",
    "    name=RUN_NAME,\n",
    "    pretrained=False,\n",
    "    seed=SEED,\n",
    "    verbose=True,\n",
    ")\n",
    "train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_metrics = model.val(\n",
    "    data=str(DATA_CONFIG_PATH),\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    conf=0.001,\n",
    "    iou=0.6,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=f'{RUN_NAME}_val',\n",
    ")\n",
    "val_metrics.results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a606d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_weights = RUNS_DIR / RUN_NAME / 'weights' / 'best.pt'\n",
    "if not best_weights.exists():\n",
    "    raise FileNotFoundError(f'Did not find trained weights at {best_weights}. Check the training output above.')\n",
    "\n",
    "predictor = YOLO(best_weights)\n",
    "predictions = predictor.predict(\n",
    "    source=str(TEST_IMG_DIR),\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    conf=0.001,\n",
    "    iou=0.6,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "submission_rows = []\n",
    "for result in predictions:\n",
    "    image_id = Path(result.path).stem.lstrip('0') or '0'\n",
    "    boxes = result.boxes\n",
    "    if boxes is None or boxes.xyxy is None or boxes.xyxy.shape[0] == 0:\n",
    "        submission_rows.append((image_id, ''))\n",
    "        continue\n",
    "    xyxy = boxes.xyxy.cpu().numpy()\n",
    "    scores = boxes.conf.cpu().numpy()\n",
    "    classes = boxes.cls.cpu().numpy().astype(int)\n",
    "    entries = []\n",
    "    for (x1, y1, x2, y2), score, cls_id in zip(xyxy, scores, classes):\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        entries.append(f'{score:.6f} {x1:.2f} {y1:.2f} {width:.2f} {height:.2f} {cls_id}')\n",
    "    submission_rows.append((image_id, ' '.join(entries)))\n",
    "\n",
    "submission_path = EXPERIMENT_DIR / 'submission.csv'\n",
    "with submission_path.open('w', newline='') as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow(['Image_ID', 'PredictionString'])\n",
    "    writer.writerows(submission_rows)\n",
    "print(f'Saved submission to {submission_path}')\n",
    "submission_rows[:5]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
